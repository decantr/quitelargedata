% !Rnw weave = knitr
\documentclass{article}

\usepackage{breakurl}
\usepackage{graphicx, verbatim}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{inconsolata}
\usepackage{float}
\usepackage{setspace}
\usepackage{graphicx,parskip,bibunits,appendix,float}
\usepackage[ruled] {algorithm2e}
\usepackage{url,amsmath,amssymb,fancybox,listings,pdfpages,caption,multicol,datetime,rotating, booktabs}
\usepackage[pagebackref=false,pdffitwindow=true]{hyperref}

\setlength{\textwidth}{6.5in} 
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in} 
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}
\setlength{\parindent}{0cm}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}

\begin{document}

\title{Coursework (CM3111) \\ Big Data Analytics}
\author{Shaw Eastwood, \href{mailto:s.eastwood@rgu.ac.uk}{s.eastwood@rgu.ac.uk}}

\maketitle

\section{Introduction}
\section{Data Exploration}
\subsection{Dataset Choice}
For the dataset I opted to use the Mushroom Classifiction dataset provided by UCI Machine Learing on Kaggle\footnote{\href{https://www.kaggle.com/uciml/mushroom-classification/}{https://www.kaggle.com/uciml/mushroom-classification/}}. I chose this dataset because of its potential practical application in predicting the edibility of a mushroom merely based on characteristic. This dataset was also featured on Kaggle and came with a very high number of reccomendations. The size of the dataset also made it a suitible choice it was a managable size with a high variance in the attributes.
\subsection{Technology Platform}
For this particular dataset, there is little merit to be gained from using Hadoop as the dataset is of a reasonable size. 
\subsection{Problem Statement \& Data Exploration}
\subsubsection{Description}
The dataset contains a list mushrooms and describes their various characteristic, listed below, along with whether or not they are poisoness or not. The aim of this report is to build a model which will predict to degree of certainty which characteristics of the mushrooms dictate the edibility of the mushrooms.

Loading in the dataset:
<<echo=TRUE, eval=TRUE>>=
setwd("~/Documents/rstuff/quitelargedata")
df <- read.csv("mushrooms.csv",header = TRUE)
@

\subsubsection{Number of Rows and Columns}
<<echo=TRUE, eval=TRUE>>=
dim(df)
cat("Number of Rows in the set is: ", nrow(df))
cat("Number of Columns/Features in the set is: ", ncol(df))
@

\subsubsection{Names of Features}
<<echo=TRUE, eval=TRUE>>=
# names of the columns/features:
names(df)
@

\subsubsection{Class / Label Distribution}
<<echo=TRUE, eval=TRUE>>=
table(df$class)
@


\subsubsection{Glance at the Data}
<<echo=TRUE, eval=TRUE>>=
# copy the first four rows into a temporary
dfs <- df[1:4,]
# delete the names of columns so the text output isn't massive
names(dfs) <- NULL
# show those lines we just copied over
head(dfs, n = 4)
@

\subsubsection{Distribution}
<<>>=
barplot(table(df$class), col = grey.colors(2), names.arg = c("edible", "poisoness"), xpd = FALSE, ylim = c(3500, 4500), offset(3000))
@

\subsubsection{Graphing}
<<>>=
library(caret)
# define the layout of the graphs
xy <- list(x=list(relation="free"), y=list(relation="free"))
# plot the data
featurePlot(sapply(df[,2:23], function (x) as.numeric(x)), # convert the values to numbers
						df$class, plot="density", scales=xy, layout = c(2,3), pch = "|")
@

As the graphs above show there is no perfect separation between the features. The best palce to find any seperation are the features odor, spore.print.color, ring.type, population, habitat. A number of the values show a very close relationship and this are of little consequence in judging the class such as, like veil.type, cap.shape, cap.color and gill.attachment.

\subsection{Pre-proccessing}
<<>>=
# the length of the unique values for each col
length(unique(is.na(df)))
@
Since our dataset doesn't have any missing values we don't need to worry about handling this.

<<>>=
unique(df$veil.type)
@
Because we only have one 'veil type' across all of our mushrooms we can disregard this column.
<<>>=
# drop the veil type from the set
df <- (df[-c(17)])
names(df)
@

Here we will turn all of the values from letters to a numerical value
<<>>=
# run the as numeric function on each value
df <- data.frame(sapply(df, function (x) as.numeric(x)))
head(df$class, n = 1) # show that class comes out as 1 and 2
df$class <- sapply(df$class, function (x) x - 1) # make class binary
@


\section{Modelling / Classification}
I will be using the Random Forest model to handle the data as it is likely to have the highest success rate in predict whether or not the mushrooms are edible or not.

\subsection{Building Model}
\subsubsection{Divide The Dataset}
<<>>=
i <- sample(nrow(df), 0.7 * nrow(df)) # get 70% of the elements

train = df[i,]
test =  df[-i,]
@



<<>>=
library(randomForest)

set.seed(77) # set the seed for reproducibility

# use randomForest on our training set
forest <- randomForest(as.factor(class) ~ . ,
                          data = train, importance = T, ntree = 50)

pre <- predict(forest, test)

sol <- data.frame(class = test$class, edible = pre)
@

\subsubsection{Accuracy}
<<>>=
print(forest)
@
Using all of the features of the dataset we are able to be one hundred percent accurate with our predictions.

<<>>=
table(pre == test$class)
@
Quickly testing whether our test dataset matches our prediction of the test dataset, which it does.

<<>>=
plot(forest)
@
The graph shows us that it takes approximately thirty tree's before the model is able to be a hundred percent accurate.


\subsubsection{Importance}
<<>>=
varImpPlot(forest)
@
This graph shows the importance of each of the features when predicting the edibility of the mushroom. From this graph we can see a number of features are unimportant to the model such as veil.color, gill.attachment, etc. these 

\section{Improving Performance}
\subsubsection{Shrinking The Input Features}
By looking at the graph of importance from our origional model we can select the most important features
<<>>=
# reduce the amount of features we give the randomForest
forSmall <- randomForest(as.factor(class) ~ odor + gill.color + gill.size +
		spore.print.color + ring.type + stalk.root + habitat + population,
	data = train)
@

We can see that this new model still maintains the same level of accuracy
<<>>=
print(forSmall) # print out the forest so we can see the accuracy
@

And if we test this model in a prediction and compare that with our test dataset
<<>>=
table(predict(forSmall, test) == test$class) # check a prediction against the test data
@
We can see it does indeed match 


\end{document}