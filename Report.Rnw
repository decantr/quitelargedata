% !Rnw weave = knitr
\documentclass{article}

\usepackage{breakurl}
\usepackage{graphicx, verbatim}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{inconsolata}
\usepackage{float}
\usepackage{setspace}
\usepackage{graphicx,parskip,bibunits,appendix,float}
\usepackage[ruled] {algorithm2e}
\usepackage{url,amsmath,amssymb,fancybox,listings,pdfpages,caption,multicol,datetime,rotating, booktabs}
\usepackage[pagebackref=false,pdffitwindow=true]{hyperref}

\setlength{\textwidth}{6.5in} 
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in} 
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}
\setlength{\parindent}{0cm}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}

\begin{document}

\title{Coursework (CM3111) \\ Big Data Analytics}
\author{Shaw Eastwood, \href{mailto:s.eastwood@rgu.ac.uk}{s.eastwood@rgu.ac.uk}}

\maketitle

\textbf{Introduction\\}
In this report I will detail my choice of dataset and the proccess of creating a model that fits the dataset.
\section{Data Exploration}
\subsection{Dataset Choice}
For the dataset I opted to use the Mushroom Classifiction dataset provided by UCI Machine Learing on Kaggle\footnote{\href{https://www.kaggle.com/uciml/mushroom-classification/}{https://www.kaggle.com/uciml/mushroom-classification/}}. I chose this dataset because of its potential practical application in predicting the edibility of a mushroom merely based on characteristics without needing to know the name, merely by describing it can it be identified. This dataset was also featured on Kaggle and came very well reccomended. The size of the dataset also made it a suitible choice it was a managable size with a high variance in the attributes.
\subsection{Technology Platform}
The use of big data exploration tools such as Hadoop seemed to be of little merit as the size of this dataset does not warrant it. The source csv is a mere 374 kB with a very manageable 8124 rows of data. In addition the variables are single letters representing certain aspects, this helps cut down on size even further and means we don't need to run any matching or regex on the fields to find our data. All of this lead me to discard any such big data technologies from consideration.
\subsection{Problem Statement \& Data Exploration}
\subsubsection{Description}
The dataset contains a list mushrooms and describes their various characteristic, listed below, along with whether or not they are poisonous or not. The aim of this report is to build a model which will predict to degree of certainty which characteristics of the mushrooms dictate the edibility of the mushrooms.

\subsubsection{Attribute Information}
\begin{quote}
classes: edible=e, poisonous=p

cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s

cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s

cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y

bruises: bruises=t,no=f

odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s

gill-attachment: attached=a,descending=d,free=f,notched=n

gill-spacing: close=c,crowded=w,distant=d

gill-size: broad=b,narrow=n

gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y

stalk-shape: enlarging=e,tapering=t

stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?

stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s

stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s

stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y

stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y

veil-type: partial=p,universal=u

veil-color: brown=n,orange=o,white=w,yellow=y

ring-number: none=n,one=o,two=t

ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z

spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y

population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y

habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d
\end{quote}\footnote{taken from the data page found here \href{https://www.kaggle.com/uciml/mushroom-classification/data}{https://www.kaggle.com/uciml/mushroom-classification/data}}


\pagebreak
Loading in the dataset:
<<echo=TRUE, eval=TRUE>>=
options(scipen = 999)
setwd('~/Documents/rstuff/quitelargedata')
df <- read.csv('mushrooms.csv',header = T)
@

\subsubsection{Number of Rows and Columns}
<<echo=TRUE, eval=TRUE>>=
dim(df)
cat('Number of Rows in the set is: ', nrow(df))
cat('Number of Columns/Features in the set is: ', ncol(df))
@

\subsubsection{Names of Features}
<<echo=TRUE, eval=TRUE>>=
# names of the columns/features:
names(df)
@

\subsubsection{Class / Label Distribution}
<<echo=TRUE, eval=TRUE>>=
table(df$class)
@


\subsubsection{Glance at the Data}
<<echo=TRUE, eval=TRUE>>=
# copy the first four rows into a temporary
dfs <- df[1:4,]
# delete the names of columns so the text output isn't massive
names(dfs) <- NULL
# show those lines we just copied over
head(dfs, n = 4)
@

\subsubsection{Distribution}
\begin{figure}[H]
\begin{center}
<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
# plot the amount of edible vs. poisonous mushrooms in the set
barplot(
	table(df$class), col = grey.colors(2), 
	names.arg = c('edible', 'poisonous'), xpd = FALSE, 
	ylim = c(3500, 4500), offset(3000), main = "Edible vs. Poisonous")
@
\caption {Amount of edible vs. poisonous in the data set}
\label{fig2}
\end {center}
\end {figure}

\subsubsection{Graphing}
<<>>=
library(caret)
# define the layout of the graphs
xy <- list(x=list(relation='free'), y=list(relation='free'))
@
\begin{figure}[H]
\begin{center}
<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide'>>=
# plot the data on a density graph
# this works by converting all of the features except the class to a number
#  then using the class as the class
featurePlot(
	sapply(df[,-c(df$class)], function (x) as.numeric(x)),
	df$class, plot='density', scales=xy, layout = c(4,6),
	pch = '|', frame = F)
@
\caption {Correlation between edible and poisonous by feature}
\label{fig3}
\end {center}
\end {figure}
As the graphs above show there is no perfect separation between the features. The best palce to find any seperation are the features odor, spore.print.color, ring.type, population, habitat. A number of the values show a very close relationship and this are of little consequence in judging the class such as, like veil.type, cap.shape, cap.color and gill.attachment.

\subsection{Pre-proccessing}
<<>>=
# the length of the unique values for each col
length(unique(is.na(df)))
@
Since our dataset doesn't have any missing values we don't need to worry about handling this.

<<>>=
unique(df$veil.type)
@
Because we only have one 'veil type' across all of our mushrooms we can disregard this column.
<<>>=
# drop the veil type from the set
df <- (df[-c(17)])
names(df)
@

Here we will turn all of the values from letters to a numerical value
<<>>=
# run the as numeric function on each value
df <- data.frame(sapply(df, function (x) as.numeric(x)))
head(df$class, n = 1) # show that class comes out as 1 and 2
df$class <- sapply(df$class, function (x) x - 1) # make class binary
@


\section{Modelling / Classification}
I will be using the Random Forest model to handle the data as it is likely to have the highest success rate in predict whether or not the mushrooms are edible or not.

\subsection{Building Model}
\subsubsection{Divide The Dataset}
Split the dataset with 70\% for training and the remainder for testing the model
<<>>=
i <- sample(nrow(df), 0.7 * nrow(df)) # get 70% of the elements

train = df[i,]
test =  df[-i,]
@

\pagebreak\subsubsection{Building the Model}
<<>>=
library(randomForest)

set.seed(77) # set the seed for reproducibility

# use randomForest on our training set
forest <- randomForest(as.factor(class) ~ . ,
                          data = train, importance = T, ntree = 50)

pre <- predict(forest, test)

sol <- data.frame(class = test$class, edible = pre)
@

\pagebreak\subsection{Testing and Evaluation}
<<>>=
print(forest)
@
Using all of the features of the dataset we are able to be one hundred percent accurate with our predictions.

<<>>=
table(pre == test$class)
@
\pagebreak
Quickly testing whether our test dataset matches our prediction of the test dataset, which it does.
\begin{figure}[H]
\begin{center}
<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
plot(forest, main = 'Error Rate For Initial Model', frame = F, col = 'blue')
@
\caption {Error rate for the inital random forest model using all features}
\label{fig4}
\end {center}
\end {figure}
The graph shows us that it takes approximately thirty tree's before the model is able to be a hundred percent accurate.

\pagebreak\subsubsection{Importance}
\begin{figure}[H]
\begin{center}
<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
varImpPlot(forest, main = 'Importance Rating by Feature', frame = F)
@
\caption {Importance Rating For Each Feature}
\label{fig5}
\end {center}
\end {figure}
This graph shows the importance of each of the features when predicting the edibility of the mushroom. From this graph we can see a number of features are unimportant to the model such as veil.color, gill.attachment, etc. these can effectively be ignored in the model without much or any effect on the accuracy of the model.

\pagebreak\subsubsection{Confusion Matrix}
<<>>=
confusionMatrix(pre, test$class)
@
We can see here that the accuracy is rated at 100\% which is supported by what we have seen from our prediction data when compared to the test data. From all of this we can be fairly sure in our assertion that our model is entirely accurate.

\pagebreak\section{Improving Performance}
\subsection{Shrinking The Input Features}
By looking at the graph of importance from our origional model we can select the most important features
<<>>=
# reduce the amount of features we give the randomForest
forSmall <- randomForest(as.factor(class) ~ odor + gill.color + gill.size +
		spore.print.color + ring.type + stalk.root + habitat + population + bruises,
	data = train, ntree = 50)
@

We can see that this new model still maintains the same level of accuracy. It is possible to achieve the same accuracy with one less feature however on some iterations of randomForest it would return with a 0.02\% error rate
<<>>=
print(forSmall) # print out the forest so we can see the accuracy
@

And if we test this model in a prediction and compare that with our test dataset
<<>>=
table(predict(forSmall, test) == test$class) # check a prediction against the test data
@
We can see it does indeed match
\pagebreak
\begin{figure}[H]
\begin{center}
<<fig=TRUE,warning=FALSE,message=FALSE,echo=TRUE, results='hide',out.width='.8\\linewidth'>>=
plot(forSmall, main = 'Error Rate With Less Features', frame = F, col = 'blue')
@
\caption {Error rate for random forest model using only the most important features}
\label{fig6}
\end {center}
\end {figure}

\subsection{Using ctree To Compare}
\subsubsection{Train The Method}
<<>>=
ct <- train(as.factor(class) ~ . , data = train, method ='ctree')
@

\subsubsection{Predict With The Model}
<<>>=
ctpre <- predict(ct, test)
@

\subsubsection{Create A Confusion Matrix}
<<>>=
confusionMatrix(ctpre, test$class)
table(test$class)
@
This shows us that the the method is not 100\% accurate though the margin of error is very small at less than 1\%.

\subsection{Change Test/Train Scale}
For this section I will reduce the training size for our random forest model to see if it still maintains a high level of accuracy. If we see a fall in accuracy then we know that our origional method was subject to overfitting and will need tuning.
\subsubsection{Build With Redifined Scale}
<<>>=
ismall <- sample(nrow(df), 0.1 * nrow(df)) # get 10% of the elements

trsmall = df[ismall,]
tesmall = df[-ismall,]
@

<<>>=
fsmall <- randomForest(as.factor(class) ~ . ,
                          data = trsmall, importance = T, ntree = 50)

presmall <- predict(forest, tesmall)

solsmall <- data.frame(class = tesmall$class, edible = presmall)
@


\subsubsection{Analyse The Results}
<<>>=
print(fsmall)
table(presmall == tesmall$class)
@
As we can see, the model still retains it's 100\% accuracy rating even with only 10\% of the the dataset for training. This means that we aren't suffering from any overfitting.


\pagebreak
\begin{figure}[H]
\begin{center}
<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
plot(fsmall, main = 'Error Rate For Smaller Training Set', frame = F, col = 'blue')
@
\caption {Error rate for random forest model using a smaller training set}
\label{fig7}
\end {center}
\end {figure}
Again, we see that around thirty descion tree's are required to hit a 100\% success rate which is in line with what we saw when using a 70/30 split.



\end{document}